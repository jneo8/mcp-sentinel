# MCP Sentinel Architecture Specification v0.2

## Overview

MCP Sentinel is an intelligent incident response system that monitors Prometheus alerts and responds through AI-driven analysis using Model Context Protocol (MCP) tools. The system combines real-time monitoring, resource-centric configuration, and LLM-powered decision making to provide automated incident investigation and response.

## System Architecture

### Core Components

```
┌─────────────────┐    ┌─────────────────┐    ┌─────────────────┐
│   Watchers      │    │    Sentinel     │    │  MCP Servers    │
│                 │    │                 │    │                 │
│ - Prometheus    │───▶│ - Notification  │───▶│ - Grafana       │
│ - (Future)      │    │   Processing    │    │ - Ceph          │
│                 │    │ - LLM Analysis  │    │ - (...)         │
└─────────────────┘    └─────────────────┘    └─────────────────┘
                                │
                                ▼
                       ┌─────────────────┐
                       │   OpenAI LLM    │
                       │                 │
                       │ - Function      │
                       │   Calling       │
                       │ - Continuous    │
                       │   Conversation  │
                       └─────────────────┘
```

### Component Architecture

**Watchers**
- Monitor external systems (Prometheus alerts)
- Apply resource-based filtering
- Generate notifications for matched alerts
- Run concurrently in separate goroutines

**Sentinel**
- Orchestrates incident response workflow
- Maps notifications to incident cards by resource name
- Manages LLM conversations with MCP tool integration
- Coordinates between all system components

**MCP Servers**
- Provide investigation and remediation tools
- Support STDIO and HTTP transport protocols
- Expose tool schemas for LLM integration
- Execute tool calls requested by LLM

**LLM Integration**
- Analyzes incident context using OpenAI models
- Uses function calling to select appropriate MCP tools
- Maintains conversation state across multiple tool executions
- Provides final analysis and recommendations

## Data Flow Architecture

### Primary Data Flow

```
Alert Source → Watcher → Notification → Sentinel → Incident Card → LLM Analysis → Tool Selection → MCP Tool Execution → Results → LLM Continuation → Final Response
```

### Detailed Flow Steps

1. **Alert Detection**
   - Watchers poll external systems (Prometheus)
   - Raw alerts are received from monitoring endpoints

2. **Resource Filtering**
   - Alerts are matched against configured resources
   - Resource filters determine relevance
   - Only matching alerts proceed to notification

3. **Notification Generation**
   - Matched alerts create notification objects
   - Notifications contain resource metadata and alert details
   - Sent via buffered channel to Sentinel

4. **Incident Card Mapping**
   - Sentinel maps notifications to incident cards by resource name
   - Each incident card defines response procedures and available tools
   - Unmapped notifications are logged and discarded

5. **LLM Tool Discovery**
   - Sentinel discovers available MCP tools from configured servers
   - Tool schemas are retrieved using MCP `ListTools` requests
   - OpenAI function definitions are generated from tool schemas

6. **LLM Conversation Initiation**
   - System prompt includes incident details and available tools
   - Initial conversation context is established
   - LLM receives incident card prompt and resource information

7. **Tool Selection and Execution**
   - LLM uses OpenAI function calling to select tools
   - Function call arguments are generated by LLM
   - MCP `CallTool` requests execute selected tools
   - Tool results are captured and formatted

8. **Conversation Continuation**
   - Tool results are fed back to LLM as function messages
   - LLM analyzes results and decides next actions
   - Process iterates until LLM provides final analysis
   - Maximum iterations prevent infinite loops

## Configuration Architecture

### Resource-Centric Design

Resources serve as the single source of truth for monitoring configuration:

```
Resources → Referenced by Watchers (what to monitor)
         → Referenced by Incident Cards (what to respond to)
```

### Configuration Hierarchy

```yaml
# System Configuration
debug: boolean
log-level: string
openai-model: string
default-max-iterations: integer

# Monitoring Configuration
resources:
  - name: string (unique identifier)
    type: string (alert type)
    filters: map (matching criteria)

watchers:
  - type: string (watcher implementation)
    resources: []string (resource references)
    endpoint: string
    poll-interval: duration

# Response Configuration
mcp-servers:
  - name: string (unique identifier)
    type: string (stdio|streamable)
    command: string (for stdio)
    url: string (for HTTP)
    auto-start: boolean

incident-cards:
  - name: string
    resource: string (resource reference)
    prompt: string (LLM context)
    tools: []string (server.tool format)
    max-iterations: integer (optional override)
```

## Communication Patterns

### Inter-Component Communication

**Watcher → Sentinel**
- Asynchronous via buffered notification channel
- Non-blocking for watcher goroutines
- Notifications contain full resource and alert context

**Sentinel → MCP Servers**
- Synchronous MCP protocol calls
- Tool discovery via `ListTools` requests
- Tool execution via `CallTool` requests
- Connection management and lifecycle

**Sentinel → LLM**
- HTTP API calls to OpenAI-compatible endpoints
- Function calling protocol for tool selection
- Conversation state maintained in message history
- Configurable models and parameters

### Data Structures

**Notification**
```
Resource: {
  Name, Type, State, Value, Timestamp
  Labels: map[string]string
  Annotations: map[string]string
}
```

**Incident Card**
```
Resource: Resource reference
Prompt: string (LLM context)
Tools: []McpTool {ServerName, ToolName}
MaxIterations: integer
```

**MCP Tool Call**
```
ServerName: string
ToolName: string
Arguments: map[string]any (LLM-generated)
```

## Concurrent Processing

### Parallel Monitoring
- Multiple watchers run independently to monitor different systems
- Each watcher operates on its own polling schedule
- Watchers are decoupled from incident processing for scalability

### Asynchronous Communication
- Watchers communicate with Sentinel via buffered notification channel
- Non-blocking notification delivery prevents monitoring delays
- Sequential incident processing maintains conversation context

## LLM Integration Architecture

### Conversation Flow

```
System Prompt → Initial Analysis → Function Call → Tool Execution → Result Integration → Continued Analysis → Final Response
```

### Function Calling Protocol

1. **Schema Discovery**: MCP tool schemas converted to OpenAI function definitions
2. **Function Registration**: Available tools registered with LLM context
3. **Function Invocation**: LLM generates function calls with arguments
4. **Execution Mapping**: Function calls mapped to MCP tool executions
5. **Result Integration**: Tool results formatted as function messages
6. **Iteration Control**: Process continues until completion or max iterations

### State Management

- Full conversation history maintained in message array
- Function call results preserve execution context
- Iteration counting prevents infinite loops
- Error states communicated back to LLM for recovery
